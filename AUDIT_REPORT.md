# Audit Report: beta_gamma_pg_fixed_latents_joint_jax.py

## Executive Summary

**Overall Status**: âœ… Implementation is algorithmically correct with minor code quality issues.

**Compilation**: âœ… Will compile once per configuration (mostly safe, with caveats below).

**Algorithm**: âœ… PG-augmented Gibbs sampler is mathematically correct for fixed latents.

---

## 1. COMPILE-ONCE ANALYSIS

### âœ… PASS: Static Shapes
- **Lines 401-410**: `S, N, p, R_h, d` computed as concrete Python ints from data shapes
- All bound in `partial()` at line 515-531 as static args `d`, `p`, `R_h`
- **Verdict**: Shapes are fixed within a run â†’ **no recompilation from shape changes**

### âœ… PASS: Static Flags
- **Line 288**: `use_pg` and `use_ard` marked as `static_argnames` in `@partial(jit, ...)`
- Bound once in `partial()` at lines 529-530
- **Verdict**: Flags are static â†’ **no recompilation when toggling within a single sampler instance**

### âœ… PASS: mu_gamma None-ness is Fixed
- **Lines 490-492**: `mu_gamma_jax` set to `None` or array before loop
- Bound in `partial()` at line 522, never changes
- **Verdict**: None-ness is fixed â†’ **no recompilation from conditional mu_gamma**

### âš ï¸ MINOR CONCERN: Shape Extraction from Traced Arrays

**Issue 1: Line 341 in `gibbs_iteration`**
```python
S = beta.shape[0]  # Extracting from traced array
key_theta, key_rest = jax.random.split(key_rest)
keys = jax.random.split(key_theta, S)  # Using extracted S
```

**Analysis**:
- `beta` is a traced array with static shape `(S, p)`
- In JAX, `beta.shape` returns concrete ints when shape is static
- `random.split(key, S)` requires concrete `S` (not traced)
- **Current behavior**: Should work because shapes are static
- **Risk**: If shapes ever become dynamic, this will fail with ConcretizationTypeError

**Recommendation**: Add `S` to `static_argnames` and pass explicitly:
```python
@partial(jit, static_argnames=['d', 'p', 'R_h', 'S', 'use_ard', 'use_pg'])
def gibbs_iteration(..., S: int, ...):
    ...
    keys = jax.random.split(key_theta, S)  # Use static S
```

**Issue 2: Line 152 in `sample_omega_pg`**
```python
S, N = psi.shape
total_samples = S * N
keys = jax.random.split(key, total_samples)
```

Same issue as above. **Recommendation**: Pass `S` and `N` as static args or use `psi.size` (which is always concrete for static shapes).

### âœ… PASS: Argument dtypes Don't Change
- **Lines 457-459**: All JAX arrays created with explicit `dtype=float64`
- **Lines 412, 427, 449**: NumPy arrays created with `dtype=float64` before conversion
- **Verdict**: dtypes are consistent â†’ **no dtype-triggered recompilation**

### ğŸ“Š COMPILE-ONCE VERDICT
**Status**: âœ… **PASS with minor improvements recommended**

Will compile once per `(use_pg_sampler, use_ard_beta)` configuration. The shape extraction issues are safe with current static shapes but could be made more robust.

---

## 2. ALGORITHMIC CORRECTNESS ANALYSIS

### âœ… PASS: Design and Sufficient Statistics

#### Predictor Construction (Lines 34-68)
```python
Z_bar = (X[None, ...] + D).mean(axis=2)  # Taper average âœ…
Z_t = Z_bar[:, :, k_idx]                 # Nearest center âœ…
Ztilt = Z_t * exp(i 2Ï€ f t)              # Phase rotation âœ…
ZR, ZI = Ztilt.real, Ztilt.imag          # Real/imag split âœ…
```
**Verdict**: âœ… Correct fixed-latent predictors from EM output

#### Design Matrix (Lines 412-415)
```python
X[:, 0] = 1.0              # Intercept âœ…
X[:, 1:1+B] = ZR           # Real parts âœ…
X[:, 1+B:1+2*B] = ZI       # Imag parts âœ…
```
**Shape**: `(N, p)` where `N = R*T`, `p = 1 + 2*B` âœ…

#### Response (Lines 449-450)
```python
Y_all = spikes.transpose(1,0,2).reshape(S, N)  # âœ… Shape (S, N)
kappa = Y_all - 0.5                            # âœ… PG augmentation offset
```

#### History Handling (Lines 425-439)
- Supports `(S, T, R_h)` and `(R, S, T, R_h)` formats âœ…
- Reshapes to `(S, N, R_h)` correctly âœ…

### âœ… PASS: PG-Augmented Normal Equations

#### Linear Predictor (Lines 126-129)
```python
psi = beta @ X.T                             # (S,p) @ (p,N) = (S,N) âœ…
psi += einsum('snr,sr->sn', H_all, gamma)    # Add history term âœ…
```

#### Omega Sampling/Approximation (Lines 133-168)
- **Mean**: `E[Ï‰|Ïˆ] = 0.5 * tanh(|Ïˆ|/2) / |Ïˆ|` âœ… Correct formula
- **Exact**: `Ï‰ ~ PG(1, Ïˆ)` via `sample_pg_single` âœ… Correct

#### Block Precision Matrix (Lines 172-228)

**A11 (Line 196)**:
```python
A11 = X^T Î© X  # via sqrt(Ï‰) weighted einsum âœ…
A11 += diag(Prec_beta)  # âœ… Diagonal prior precision
```

**A12 (Line 208)**: `X^T Î© H` âœ…

**A22 (Lines 211-212)**:
```python
A22 = H^T Î© H  âœ…
A22 += Prec_gamma  âœ…
```

**RHS (Lines 201, 215-217)**:
```python
b1 = X^T Îº  âœ…
b2 = H^T Îº + Prec_gamma @ mu_gamma  âœ… (if mu_gamma provided)
```

**Block Assembly (Lines 220-226)**:
```python
A = [A11  A12]  âœ…
    [A21  A22]
b = [b1, b2]    âœ…
```

### âœ… PASS: Î²/Î³ Blocked Draw (Lines 231-255)

```python
A_sym = 0.5 * (A + A.T)                    # Symmetrize âœ…
A_reg = A_sym + 1e-8 * I                   # Jitter for stability âœ…
L L^T = A_reg                              # Cholesky âœ…
Î¼ = A^{-1} b                               # via 2 triangular solves âœ…
Î¸ = Î¼ + L^{-T} Îµ where Îµ ~ N(0,I)         # Sample âœ…
```

**Verdict**: âœ… Correct Gaussian sampler with numerical stability

### âš ï¸ DESIGN QUESTION: ARD Implementation (Lines 262-285)

#### Current Behavior: Per-Unit, Per-Feature ARD

```python
# Lines 270-271: Operates on each unit separately
b_lat = beta[:, 1:]  # (S, p-1) - all units, excluding intercept

# Lines 277-283: Samples Ï„Â²_{s,j} for EACH unit s, feature j
tauÂ²_{s,j} ~ InvGamma(a0 + 0.5, b0 + 0.5 * Î²_{s,j}Â²)
```

**This means**:
- Unit 1, Feature 1 has its own variance Ï„Â²_{1,1}
- Unit 2, Feature 1 has its own variance Ï„Â²_{2,1}
- These are INDEPENDENT across units

**Alternative Design** (more common):
- Feature 1 has shared variance Ï„Â²_1 across all units
- Feature 2 has shared variance Ï„Â²_2 across all units
- Variance is feature-wise, not unit-specific

**Question for User**: Is per-unit ARD the intended design, or should variance be shared across units for each feature?

**Mathematical Correctness**: Current implementation is correct for per-unit ARD. The IG update is:
```
a_post = a0 + 0.5 âœ…
b_post = b0 + 0.5 * Î²Â² âœ…
Ï„Â² = b_post / Gamma(a_post, 1) âœ… (equivalent to InvGamma draw)
```

### ğŸ› CODE QUALITY ISSUE: Redundant ARD Initialization (Lines 470-473)

```python
if cfg.use_ard_beta:
    Prec_beta_all = jnp.broadcast_to(Prec_beta_base, (S, p))
else:
    Prec_beta_all = jnp.broadcast_to(Prec_beta_base, (S, p))
```

**Issue**: Both branches are identical!

**Root Cause**: ARD logic is controlled by `lax.cond(use_ard, ...)` in `gibbs_iteration` (lines 360-365), not by initialization.

**Recommendation**: Simplify to:
```python
Prec_beta_all = jnp.broadcast_to(Prec_beta_base, (S, p))
```

**Impact**: No correctness issue, just dead code.

---

## 3. NUMERICAL STABILITY

### âœ… PASS: All Guards in Place

- **Line 242**: Symmetrization before Cholesky âœ…
- **Line 242**: `1e-8 * I` jitter for ill-conditioned matrices âœ…
- **Line 168**: `omega_floor` prevents zero weights âœ…
- **Line 353**: `max(tau2, 1e-12)` prevents division by zero âœ…
- **Line 139**: `max(|Ïˆ|, 1e-12)` prevents division by zero âœ…
- **Line 140**: `clip(|Ïˆ|, 0, 50)` prevents overflow in tanh âœ…

---

## 4. WHAT THIS DOES (AND DOESN'T) DO

### âœ… What It Does
1. **Correct PG-Gibbs for fixed latents**: Properly samples (Î², Î³) | Ï‰, Y with PG augmentation
2. **Vectorized over units**: All S units updated in parallel (efficient)
3. **Flexible priors**: Supports both scalar and matrix priors for Î³
4. **Numerically stable**: Guards against common pitfalls

### âš ï¸ What It Doesn't Do (by Design)
1. **Not true sampling when `use_pg_sampler=False`**: Uses Ï‰ = E[Ï‰|Ïˆ] (deterministic), making this a mode-finding algorithm, not MCMC
2. **No cross-unit shrinkage**: Each unit's Î² is independent (unless you want to add hierarchical priors later)
3. **Standardization breaks interpretation**: If `standardize_reim=True`, Î²R/Î²I are no longer in PLV/phase units

---

## 5. RECOMMENDATIONS

### Critical: None

### Important
1. **Clarify ARD design**: Confirm per-unit ARD is intended (vs. shared feature-wise variance)

### Code Quality
1. **Remove dead code** (lines 470-473): Both branches identical
2. **Add `S` to static args** (lines 288, 144): Make shape extraction explicit
3. **Document per-unit ARD**: Add comment explaining variance is not shared across units

### Documentation
1. Add docstring warning that `use_pg_sampler=False` is not true MCMC
2. Document that `standardize_reim=True` changes interpretation of coefficients

---

## 6. FINAL VERDICT

| Criterion | Status | Notes |
|-----------|--------|-------|
| **Compile-once** | âœ… PASS | Minor shape-extraction concerns (non-critical) |
| **Algorithm** | âœ… PASS | Mathematically correct PG-Gibbs |
| **Numerics** | âœ… PASS | All stability guards in place |
| **Code Quality** | âš ï¸ MINOR | Dead code in ARD initialization |

**Recommendation**: âœ… **Safe to use** with awareness of ARD design choice.

---

## Appendix: Test Checklist

To verify compile-once behavior, run:

```python
# Should compile on first call, reuse on second
trace1 = sample_beta_gamma_from_fixed_latents_joint(..., cfg=cfg)  # Compiles
trace2 = sample_beta_gamma_from_fixed_latents_joint(..., cfg=cfg)  # Reuses

# Changing PG mode triggers new compile (expected)
cfg.use_pg_sampler = True
trace3 = sample_beta_gamma_from_fixed_latents_joint(..., cfg=cfg)  # New compile

# Same PG mode reuses
trace4 = sample_beta_gamma_from_fixed_latents_joint(..., cfg=cfg)  # Reuses
```

Monitor with: `JAX_LOG_COMPILES=1 python your_script.py`
